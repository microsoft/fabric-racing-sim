{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab53e2f-8004-49c9-8c8c-7748022f48ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:33:03.8660364Z",
       "execution_start_time": "2025-07-17T10:32:55.0762822Z",
       "normalized_state": "finished",
       "parent_msg_id": "e88fa624-d344-496a-9e98-5b6fd15c94c4",
       "queued_time": "2025-07-17T10:32:49.7441908Z",
       "session_id": "cd27ca72-d948-4806-9b8a-b22d2c74656c",
       "session_start_time": "2025-07-17T10:32:49.7450391Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ms-fabric-cli --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18bd04-fc22-4651-9380-27cf950a35e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aca1a9-4941-4398-baf1-6f2be35d9f90",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Environment Variables\n",
    "\n",
    "- Add the target Workspace in workspace_name. It will use the target Workspace or create it.\n",
    "- Add the Capacity name in capacity_name. It will be used only if the workspace doesnÂ´t exist and for the capacity assignment to that workspace.\n",
    "- Add the target Eventhouse name in eventhouse_name. Can be an existing Eventhouse. If blank will use detault name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e4e0d-7813-4e19-b026-8cce0ee4f988",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:33:04.2078653Z",
       "execution_start_time": "2025-07-17T10:33:03.867213Z",
       "normalized_state": "finished",
       "parent_msg_id": "36d67082-1871-415e-b345-86e017a342ae",
       "queued_time": "2025-07-17T10:32:49.7472686Z",
       "session_id": "cd27ca72-d948-4806-9b8a-b22d2c74656c",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workspace_name = \"Racing Sim\"\n",
    "capacity_name= \"\" \n",
    "eventhouse_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f305d-3afc-4466-ad88-f246ef136719",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Source Git Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860d419-91c1-4840-9d3d-7a516750271d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:33:04.550316Z",
       "execution_start_time": "2025-07-17T10:33:04.2089885Z",
       "normalized_state": "finished",
       "parent_msg_id": "9fdbe4a0-72d2-42b5-ac90-93b27c145934",
       "queued_time": "2025-07-17T10:32:49.7488211Z",
       "session_id": "cd27ca72-d948-4806-9b8a-b22d2c74656c",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### DO NET CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"microsoft\"\n",
    "repo_name = \"fabric-racing-sim\"\n",
    "branch = \"main\"\n",
    "folder_prefix = \"\"\n",
    "github_token = \"\"\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65073be-7bb0-44a9-9bdf-c1334d166283",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96e880-7c1b-4ed5-9ea9-eb4055d41537",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Definition of deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee5a660-9826-4865-ac50-ddf8ce578148",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:33:11.6034408Z",
       "execution_start_time": "2025-07-17T10:33:04.5516867Z",
       "normalized_state": "finished",
       "parent_msg_id": "55e08ecf-1b75-4614-ae90-efdfc02203e8",
       "queued_time": "2025-07-17T10:32:49.7668251Z",
       "session_id": "cd27ca72-d948-4806-9b8a-b22d2c74656c",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "class FabDeployCLI:\n",
    "    src_workspace_id = \"\"\n",
    "    src_workspace_name = \"\"\n",
    "    trg_workspace_id = \"\"\n",
    "    trg_workspace_name = \"\"\n",
    "    deployment_order = []\n",
    "    mapping_table =  []\n",
    "    workspace_name = \"\"\n",
    "    capacity_name = \"\"\n",
    "    eventhouse_name = \"\"\n",
    "    repo_owner = \"\"\n",
    "    repo_name = \"\"\n",
    "    branch = \"\"\n",
    "    folder_prefix = \"\"\n",
    "    github_token = \"\"\n",
    "\n",
    "    def __download_folder_as_zip(self, repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\", github_token = None):\n",
    "        # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "        url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "        headers = None\n",
    "\n",
    "        if github_token is not None:\n",
    "        # Replace with your actual GitHub token\n",
    "            headers = {\n",
    "                \"Authorization\": f\"token {github_token}\",\n",
    "                \"Accept\": \"application/vnd.github.v3+json\"\n",
    "            }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        folder_to_extract = f\"/{folder_to_extract}\" if folder_to_extract[0] != \"/\" else folder_to_extract\n",
    "        \n",
    "        # Ensure the directory for the output zip file exists\n",
    "        os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "        \n",
    "        # Create a zip file in memory\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "            with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "                for file_info in zipf.infolist():\n",
    "                    parts = file_info.filename.split('/')\n",
    "                    if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                        # Extract only the specified folder\n",
    "                        file_data = zipf.read(file_info.filename)  \n",
    "                        if folder_prefix != \"\":\n",
    "                            parts.remove(remove_folder_prefix)\n",
    "                        output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "\n",
    "    def __uncompress_zip_to_folder(self, zip_path, extract_to):\n",
    "        # Ensure the directory for extraction exists\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        \n",
    "        # Uncompress all files from the zip into the specified folder\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        \n",
    "        # Delete the original zip file\n",
    "        os.remove(zip_path)\n",
    "\n",
    "    def __run_fab_command(self, command, capture_output: bool = False, silently_continue: bool = False, raw_output: bool = False):\n",
    "        result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "        if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "            raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")    \n",
    "        if (capture_output and not raw_output): \n",
    "            output = result.stdout.strip()\n",
    "            return output\n",
    "        elif (capture_output and raw_output):\n",
    "            return result\n",
    "\n",
    "    def __fab_get_workspace_id(self, name):\n",
    "        result = self.__run_fab_command(f\"get /{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "        return result\n",
    "\n",
    "    def __fab_workspace_exists(self, name):\n",
    "        id = self.__run_fab_command(f\"get /{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "        return(id)\n",
    "\n",
    "    def __fab_get_id(self, name):\n",
    "        id = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "        return(id)\n",
    "\n",
    "    def __fab_get_item(self, name):\n",
    "        item = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name}\" , capture_output = True, silently_continue= True)\n",
    "        return(item)\n",
    "\n",
    "    def fab_get_eventstream_connection_string(self, name, connection_name):\n",
    "        connection_id = \"\"\n",
    "        item_id = self.__fab_get_id(name)\n",
    "\n",
    "        item = self.__run_fab_command(f\"api -X get /workspaces/{self.trg_workspace_id}/eventstreams/{item_id}/topology\" , capture_output = True, silently_continue= True)\n",
    "        topology = json.loads(item)\n",
    "\n",
    "        sources = topology.get(\"text\",{}).get(\"sources\",[])\n",
    "        source_id = list(filter(lambda source: source[\"name\"] == connection_name, sources))\n",
    "        if len(source_id):\n",
    "            connection_id = source_id[0].get(\"id\")\n",
    "\n",
    "        destinations = topology.get(\"text\",{}).get(\"destinations\",[])\n",
    "        destination_id = list(filter(lambda destination: destination[\"name\"] == connection_name, destinations))\n",
    "        if len(destination_id):\n",
    "            connection_id = destination_id[0].get(\"id\")\n",
    "\n",
    "        connection = self.__run_fab_command(f\"api -X get /workspaces/{self.trg_workspace_id}/eventstreams/{item_id}/sources/{connection_id}/connection\" , capture_output = True, silently_continue= True)\n",
    "        connection = json.loads(connection)\n",
    "        connection = connection.get(\"text\",{}).get(\"accessKeys\",{}).get(\"primaryConnectionString\")\n",
    "        return(connection)\n",
    "\n",
    "    def __fab_get_display_name(self, name):\n",
    "        display_name = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q displayName\" , capture_output = True, silently_continue= True)\n",
    "        return(display_name)\n",
    "\n",
    "    def __fab_get_kusto_query_uri(self, name):\n",
    "        connection = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q properties.queryServiceUri -f\", capture_output = True, silently_continue= True)\n",
    "        connection = connection.split(\"\\n\")[1]\n",
    "        return(connection)\n",
    "\n",
    "    def __fab_get_kusto_ingest_uri(self, name):\n",
    "        connection = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q properties.ingestionServiceUri -f\", capture_output = True, silently_continue= True)\n",
    "        connection = connection.split(\"\\n\")[1]\n",
    "        return(connection)\n",
    "\n",
    "    def __fab_get_folders(self):\n",
    "        response = self.__run_fab_command(f\"api workspaces/{self.trg_workspace_id}/folders\", capture_output = True, silently_continue= True)\n",
    "        return(json.loads(response).get('text',{}).get('value',[]))\n",
    "\n",
    "    def __fab_add_schedule(self, name):\n",
    "        item = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q schedules\" , capture_output = True, silently_continue= True)\n",
    "\n",
    "        if len(json.loads(item)) == 0:\n",
    "            schedule = self.__get_schedule_by_name(name)\n",
    "\n",
    "            return self.__run_fab_command(f\"job run-sch /{self.trg_workspace_name}/{name} -i {json.dumps(schedule)}\" , capture_output = True, silently_continue=True)\n",
    "\n",
    "        return f\"\"\"Job schedule for '{name}' already exists...\n",
    "    * Job schedule {item}\"\"\" \n",
    "\n",
    "    def __get_id_by_name(self, name):\n",
    "        for it in self.deployment_order:\n",
    "            if it.get(\"name\") == name:\n",
    "                    return it.get(\"id\")\n",
    "        return None\n",
    "\n",
    "    def __get_schedule_by_name(self, name):\n",
    "        for it in self.deployment_order:\n",
    "            if it.get(\"name\") == name:\n",
    "                    return it.get(\"schedule\")\n",
    "        return None\n",
    "\n",
    "    def __copy_to_tmp(self, name,child=None,type=None):\n",
    "        child_path = \"\" if child is None else f\".children/{child}/\"\n",
    "        type_path = \"\" if type is None else f\"{type}/\"\n",
    "        shutil.rmtree(\"./builtin/tmp\",  ignore_errors=True)\n",
    "        path2zip = \"./builtin/src/src.zip\"\n",
    "        with  ZipFile(path2zip) as archive:\n",
    "            for file in archive.namelist():\n",
    "                if file.startswith(f'src/{type_path}{name}/{child_path}'):\n",
    "                    archive.extract(file, './builtin/tmp')\n",
    "        return(f\"./builtin/tmp/src/{type_path}{name}/{child_path}\" )\n",
    "\n",
    "    def __get_mapping_table_new_from_type(self, type):\n",
    "        result = \"\"\n",
    "        filtered_data = list(filter(lambda item: item['Type'] == type, self.mapping_table))\n",
    "        if len(filtered_data) > 0:\n",
    "            result=filtered_data[0][\"new\"]\n",
    "        return result\n",
    "\n",
    "    def __get_mapping_table_new_from_old(self, old):\n",
    "        result = \"\"\n",
    "        filtered_data = list(filter(lambda item: item['old'] == old, self.mapping_table))\n",
    "        if len(filtered_data) > 0:\n",
    "            result=filtered_data[0][\"new\"]\n",
    "        return result\n",
    "\n",
    "    def __get_mapping_table_new_from_type_item(self, type,item):\n",
    "        result = \"\"\n",
    "        filtered_data = list(filter(lambda table: table[\"Type\"] == type and table[\"Item\"] == item, self.mapping_table))\n",
    "        if len(filtered_data) > 0:\n",
    "            result=filtered_data[0][\"new\"]\n",
    "        return result\n",
    "\n",
    "    def __get_mapping_table_parent_type(self, type,item,parent_type):\n",
    "        parent_item = self.__get_mapping_table_new_from_type_item(type,item)\n",
    "        result = self.__get_mapping_table_new_from_type_item(parent_type,parent_item)\n",
    "        return result\n",
    "\n",
    "    def __replace_ids_in_folder(self, folder_path, mapping_table):\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.py', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        content = file.read()\n",
    "                        for mapping in mapping_table:  \n",
    "                            content = content.replace(mapping[\"old\"], mapping[\"new\"])\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        file.write(content)\n",
    "\n",
    "    def __replace_kqldb_parent_eventhouse(self, folder_path,parent_eventhouse):\n",
    "        property_file = f\"{folder_path}/DatabaseProperties.json\"\n",
    "        with open(property_file, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            content[\"parentEventhouseItemId\"] = self.__fab_get_id(parent_eventhouse)\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_eventstream_destination(self, folder_path,it_destinations):\n",
    "        property_file = f\"{folder_path}/eventstream.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            destinations = content.get(\"destinations\",[])\n",
    "            for destination in destinations:\n",
    "                if destination.get(\"type\") != \"CustomEndpoint\":\n",
    "                    filtered_data = list(filter(lambda table: table[\"name\"] == destination.get(\"name\") and table[\"type\"] == destination.get(\"type\"), it_destinations))\n",
    "                    if len(filtered_data) > 0:        \n",
    "                        destination[\"properties\"][\"workspaceId\"] = self.__get_mapping_table_new_from_type_item(\"Workspace Id\",self.trg_workspace_name)\n",
    "                        destination[\"properties\"][\"itemId\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                        if destination.get(\"properties\",{}).get(\"databaseName\") is not None:\n",
    "                            destination[\"properties\"][\"databaseName\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB Name\",filtered_data[0].get(\"itemName\"))\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_kqldashboard_datasources(self, folder_path,it_datasources):\n",
    "        property_file = f\"{folder_path}/RealTimeDashboard.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            datasources = content.get(\"dataSources\",[])\n",
    "            for datasource in datasources:\n",
    "                filtered_data = list(filter(lambda table: table[\"name\"] == datasource.get(\"name\"), it_datasources))\n",
    "                if len(filtered_data) > 0:        \n",
    "                    datasource[\"workspace\"] = self.__get_mapping_table_new_from_type_item(\"Workspace Id\",self.trg_workspace_name)\n",
    "                    datasource[\"database\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                    datasource[\"clusterUri\"] = self.__get_mapping_table_parent_type(\"KQL DB Eventhouse\",filtered_data[0].get(\"itemName\"),\"Kusto Query Uri\")\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_kqlqueryset_datasources(self, folder_path,it_datasources):\n",
    "        property_file = f\"{folder_path}/RealTimeQueryset.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            datasources = content.get(\"queryset\",{}).get(\"dataSources\",[])\n",
    "            for datasource in datasources:\n",
    "                filtered_data = list(filter(lambda table: str(table[\"itemName\"]).replace(\".KQLDatabase\",\"\") == datasource.get(\"databaseItemName\"), it_datasources))\n",
    "                if len(filtered_data) > 0:        \n",
    "                    datasource[\"databaseItemId\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                    datasource[\"clusterUri\"] = self.__get_mapping_table_parent_type(\"KQL DB Eventhouse\",filtered_data[0].get(\"itemName\"),\"Kusto Query Uri\")\n",
    "                    print(content.get(\"queryset\",{}).get(\"dataSources\",[]))\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_pbi_report_definition(self, folder_path,datasource):\n",
    "        property_file = f\"{folder_path}/definition.pbir\"\n",
    "        sm_name = datasource.replace(\".SemanticModel\",\"\")\n",
    "        ws_id = self.__get_mapping_table_new_from_type(\"Workspace Id\")\n",
    "        sm_id = self.__get_mapping_table_new_from_type_item(\"Semantic Model ID\",datasource)\n",
    "        pbir_definition = {\n",
    "            \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definitionProperties/1.0.0/schema.json\",\n",
    "            \"version\": \"4.0\",\n",
    "            \"datasetReference\": {\n",
    "                \"byPath\": None,\n",
    "                \"byConnection\": {\n",
    "                \"connectionString\": f\"Data Source=powerbi://api.powerbi.com/v1.0/myorg/{ws_id};Initial Catalog={sm_name};Integrated Security=ClaimsToken\",\n",
    "                \"pbiServiceModelId\": None,\n",
    "                \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n",
    "                \"pbiModelDatabaseName\": sm_id,\n",
    "                \"connectionType\": \"pbiServiceXmlaStyleLive\",\n",
    "                \"name\": \"EntityDataSource\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(pbir_definition,file,indent=4)\n",
    "        print(pbir_definition)\n",
    "\n",
    "    def __deploy_item(self, name,child=None,it=None):\n",
    "        parent = \"\"\n",
    "        cli_parameter = \"\"\n",
    "\n",
    "        # Copy and replace IDs in the item\n",
    "        tmp_path = self.__copy_to_tmp(name,child,it.get(\"type\"))\n",
    "        \n",
    "        if child is not None:\n",
    "            parent = name\n",
    "            name = child     \n",
    "\n",
    "        if \".KQLDatabase\" in name:\n",
    "            if child is not None:\n",
    "                parent = parent if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\"\n",
    "            if it[\"parent\"] is not None:\n",
    "                parent = it[\"parent\"] if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\"\n",
    "            self.mapping_table.append({\"Type\": \"KQL DB Eventhouse\", \"Item\": name, \"old\": it[\"parent\"], \"new\": parent })  \n",
    "            self.__replace_kqldb_parent_eventhouse(tmp_path,parent)\n",
    "        elif \".Eventhouse\" in name:\n",
    "            name = name if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\"\n",
    "        elif \".Eventstream\" in name:\n",
    "            self.__replace_eventstream_destination(tmp_path,it[\"destinations\"]) \n",
    "        elif \".Notebook\" in name:\n",
    "            cli_parameter = cli_parameter + \" --format .py\"\n",
    "            self.__replace_ids_in_folder(tmp_path, self.mapping_table)  \n",
    "        elif \".DataPipeline\" in name:\n",
    "            self.__replace_ids_in_folder(tmp_path, self.mapping_table)  \n",
    "        elif \".SemanticModel\" in name:\n",
    "            self.__replace_ids_in_folder(tmp_path, self.mapping_table)\n",
    "        elif \".KQLDashboard\" in name:\n",
    "            self.__replace_kqldashboard_datasources(tmp_path, it[\"datasources\"])\n",
    "        elif \".KQLQueryset\" in name:\n",
    "            self.__replace_kqlqueryset_datasources(tmp_path, it[\"datasources\"])\n",
    "        elif \".Report\" in name:\n",
    "            self.__replace_pbi_report_definition(tmp_path,it[\"datasource\"])\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"#############################################\")\n",
    "        print(f\"Deploying {name}\")      \n",
    "        \n",
    "        self.__run_fab_command(f\"import  /{self.trg_workspace_name}/{name} -i {tmp_path} -f {cli_parameter} \", silently_continue= True)\n",
    "\n",
    "        new_id = self.__fab_get_id(name)\n",
    "\n",
    "        if \".KQLDatabase\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"KQL DB ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Eventhouse\" in name:\n",
    "            query_uri = self.__fab_get_kusto_query_uri(name)\n",
    "            ingest_uri = self.__fab_get_kusto_ingest_uri(name)\n",
    "            self.mapping_table.append({\"Type\": \"Kusto Query Uri\", \"Item\": name, \"old\": it[\"kustoQueryUri\"], \"new\": query_uri })        \n",
    "            self.mapping_table.append({\"Type\": \"Kusto Ingest Uri\", \"Item\": name, \"old\": it[\"kustoIngestUri\"], \"new\": ingest_uri })\n",
    "            self.mapping_table.append({\"Type\": \"Eventhouse ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Eventstream\" in name:\n",
    "            if it.get(\"ConnectionStringCustomEndpoint\") is not None:\n",
    "                self.mapping_table.append({\"Type\": \"Connection String Evenstream\", \"Item\": name, \"old\": it[\"ConnectionStringCustomEndpoint\"], \"new\": self.fab_get_eventstreamConnectionString(name,it[\"ConnectionStringCustomEndpoint\"]) })\n",
    "            self.mapping_table.append({\"Type\": \"Eventstream ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Notebook\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Notebook ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".DataPipeline\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Pipeline ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Report\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Report ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".SemanticModel\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Semantic Model ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".KQLDashboard\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"KQLDashboard ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "\n",
    "    def __init__(self, repo_owner, repo_name, branch, folder_prefix, github_token):\n",
    "        \n",
    "        # Set environment parameters for Fabric CLI\n",
    "        token = notebookutils.credentials.getToken('pbi')\n",
    "        os.environ['FAB_TOKEN'] = token\n",
    "        os.environ['FAB_TOKEN_ONELAKE'] = token  \n",
    "\n",
    "        self.repo_owner = repo_owner\n",
    "        self.repo_name = repo_name\n",
    "        self.branch = branch\n",
    "        self.folder_prefix = folder_prefix\n",
    "        self.github_token = github_token   \n",
    "      \n",
    "\n",
    "    def run(self, workspace_name, capacity_name= \"\", eventhouse_name = \"\", exclude = [], type_exclude = []):\n",
    "\n",
    "        self.__download_folder_as_zip(self.repo_owner, self.repo_name, output_zip = \"./builtin/src/src.zip\", branch = self.branch, folder_to_extract= f\"{folder_prefix}/src\", remove_folder_prefix = f\"{self.folder_prefix}\", github_token=self.github_token)\n",
    "        self.__download_folder_as_zip(self.repo_owner, self.repo_name, output_zip = \"./builtin/config/config.zip\", branch = self.branch, folder_to_extract= f\"{folder_prefix}/config\" , remove_folder_prefix = f\"{self.folder_prefix}\", github_token=self.github_token)\n",
    "        self.__uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "\n",
    "        base_path = './builtin/'\n",
    "\n",
    "        deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n",
    "        with open(deploy_order_path, 'r') as file:\n",
    "                self.deployment_order = json.load(file)\n",
    "\n",
    "        #deploy workspace idempotent\n",
    "        if \"NotFound\" in self.__fab_workspace_exists(f\"{workspace_name}.Workspace\"):\n",
    "            if capacity_name == \"\" or capacity_name is None:\n",
    "                raise \"Workspace doesnÂ´t exist and capacity_name not provided\"\n",
    "            self.__run_fab_command(f\"mkdir {workspace_name}.Workspace -P capacityname={capacity_name}.Capacity\")\n",
    "            print(f\"New Workspace Create\")\n",
    "\n",
    "        self.src_workspace_name = \"Workspace.src\"\n",
    "        self.src_workspace_id = self.__get_id_by_name(self.src_workspace_name)\n",
    "\n",
    "        self.trg_workspace_id = self.__fab_get_workspace_id(f\"{workspace_name}.Workspace\")\n",
    "        self.trg_workspace_name = f\"{workspace_name}.Workspace\"\n",
    "\n",
    "        print(f\"Target Workspace Id: {self.trg_workspace_id}\")\n",
    "        print(f\"Target Workspace Name: {self.trg_workspace_name}\")\n",
    "\n",
    "        self.mapping_table.append({\"Type\": \"Workspace Id\", \"Item\": self.trg_workspace_name, \"old\": self.__get_id_by_name(self.src_workspace_name), \"new\": self.trg_workspace_id })\n",
    "        self.mapping_table.append({\"Type\": \"Workspace Blank Id\", \"Item\": self.trg_workspace_name, \"old\": \"00000000-0000-0000-0000-000000000000\", \"new\": self.trg_workspace_id })\n",
    "        self.mapping_table.append({\"Type\": \"Workspace Name\", \"Item\": self.trg_workspace_name, \"old\": self.src_workspace_name, \"new\": self.trg_workspace_name.replace(\".Workspace\", \"\") })\n",
    "\n",
    "        exclude = exclude + [self.src_workspace_name]\n",
    "\n",
    "        for it in self.deployment_order:\n",
    "            new_id = None            \n",
    "            name = it[\"name\"]\n",
    "            type = it.get(\"type\")\n",
    "\n",
    "            if name in exclude:\n",
    "                continue    \n",
    "            \n",
    "            if type in type_exclude:\n",
    "                continue\n",
    "\n",
    "            self.__deploy_item(name,None,it)\n",
    "\n",
    "            for child in it.get(\"children\",[]):\n",
    "                child_name = child[\"name\"]\n",
    "                self.__deploy_item(name,child_name,child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c66dd-47bb-4394-a8b7-3772af6dd0ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment Logic\n",
    "This part iterates through all the items, gets the respective source code, replaces all IDs dynamically and deploys the new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87eaaac6-14e5-47c8-99f2-66bcdb308e00",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:33:11.9979157Z",
       "execution_start_time": "2025-07-17T10:33:11.6046921Z",
       "normalized_state": "finished",
       "parent_msg_id": "6e6576d6-3771-44fe-8bcd-2717c7b4dd83",
       "queued_time": "2025-07-17T10:32:49.7683015Z",
       "session_id": "cd27ca72-d948-4806-9b8a-b22d2c74656c",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exclude = []\n",
    "type_exclude = []\n",
    "\n",
    "fabDeployCLI = FabDeployCLI(repo_owner, repo_name, branch, folder_prefix, github_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766b53e8-3dde-4213-9093-84e3b21cb830",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": "2025-07-17T10:33:11.9996467Z",
       "normalized_state": "running",
       "parent_msg_id": "d0ff866f-39bf-4298-aa50-6441c79992ec",
       "queued_time": "2025-07-17T10:32:49.791792Z",
       "session_id": "cd27ca72-d948-4806-9b8a-b22d2c74656c",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Workspace Id: e5669cff-4a44-436c-b6ba-2c8cb007fad1\n",
      "Target Workspace Name: Racing Sim [Test].Workspace\n",
      "\n",
      "#############################################\n",
      "Deploying RacingSim.Eventhouse\n",
      "! An item with the same name exists\n",
      "Importing (update) './builtin/tmp/src/RacingSim.Eventhouse/' â '/Racing Sim [Test].Workspace/RacingSim.Eventhouse'...\n",
      "* 'RacingSim.Eventhouse' imported\n",
      "\n",
      "#############################################\n",
      "Deploying RacingSim.KQLDatabase\n",
      "! An item with the same name exists\n",
      "Importing (update) './builtin/tmp/src/RacingSim.KQLDatabase/' â '/Racing Sim [Test].Workspace/RacingSim.KQLDatabase'...\n"
     ]
    }
   ],
   "source": [
    "fabDeployCLI.run(exclude=exclude, type_exclude=type_exclude, workspace_name=workspace_name, capacity_name=capacity_name, eventhouse_name=eventhouse_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cddc4b-a1a7-4812-8bb5-23e77b8578c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "normalized_state": "waiting",
       "parent_msg_id": "d3c6e4ba-dece-4c87-9828-f1f01240ed8f",
       "queued_time": "2025-07-17T10:32:49.7944073Z",
       "session_id": null,
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fabDeployCLI.mapping_table)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
